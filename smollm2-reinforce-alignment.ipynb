{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19ee85c",
   "metadata": {},
   "source": [
    "*Загрузим необходимые пакеты*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15752129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~andb (/home/hxrt_mx/ml/notebooks/smollm2-reinforce-alignment/.venv/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andb (/home/hxrt_mx/ml/notebooks/smollm2-reinforce-alignment/.venv/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andb (/home/hxrt_mx/ml/notebooks/smollm2-reinforce-alignment/.venv/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andb (/home/hxrt_mx/ml/notebooks/smollm2-reinforce-alignment/.venv/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andb (/home/hxrt_mx/ml/notebooks/smollm2-reinforce-alignment/.venv/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q lightning transformers datasets trl wandb transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f14669",
   "metadata": {},
   "source": [
    "# *Level 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceebc9d0",
   "metadata": {},
   "source": [
    "*Установим модель и датасет*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df3d3f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b33e8b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at HuggingFaceTB/SmolLM2-135M-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "\tnum_labels=1,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0baa98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score', 'chosen_rationale', 'rejected_rationale', 'score_diff', 'difficulty'],\n",
       "        num_rows: 7224\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score', 'chosen_rationale', 'rejected_rationale', 'score_diff', 'difficulty'],\n",
       "        num_rows: 373\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"juyoungml/HelpSteer2-binarized\"\n",
    "\n",
    "ds = load_dataset(dataset_name)\n",
    "\n",
    "train_ds = ds[\"train\"]\n",
    "val_ds = ds[\"validation\"]\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec77a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_ds(example):\n",
    "\tuser_msg = example['prompt']\n",
    "\tchosen = example['chosen']\n",
    "\trejected = example['rejected']\n",
    "\n",
    "\tprompt = f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "\treturn {\n",
    "\t\t\"chosen\": f\"{prompt}{chosen}<|im_end|>\",\n",
    "\t\t\"rejected\": f\"{prompt}{rejected}<|im_end|>\",\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82dcaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(format_ds, remove_columns=train_ds.column_names)\n",
    "val_ds = val_ds.map(format_ds, remove_columns=train_ds.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86cf4cf",
   "metadata": {},
   "source": [
    "*Перейдем к обучению reward модели*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6a2e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import RewardTrainer, RewardConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39b53a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_conf = RewardConfig(\n",
    "    output_dir=\"reward_models/SmolLM2_135_classifier\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    max_length=768,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=96,\n",
    ")\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=reward_conf,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b90885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()\n",
    "# trainer.save_model(reward_conf.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9691bb",
   "metadata": {},
   "source": [
    "*Перейдем к реализации reinforce \\w baseline и дообучению sft с помощью него*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5029c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning import LightningModule, LightningDataModule, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc723a8d",
   "metadata": {},
   "source": [
    "*Воспользуемся lightning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22f7aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceDataModule(LightningDataModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tdataset,\n",
    "\t\ttokenizer,\n",
    "\t\tbatch_size,\n",
    "\t\tmax_prompt_length,\n",
    "\t\tnum_workers,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.ds = dataset\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.max_prompt_length = max_prompt_length\n",
    "\t\tself.num_workers = num_workers\n",
    "\n",
    "\tdef setup(self, stage=None):\n",
    "\t\tself.train_dataset = self.ds[\"train\"]\n",
    "\t\tself.val_dataset = self.ds[\"validation\"]\n",
    "\n",
    "\tdef _collate_fn(self, batch):\n",
    "\t\tprompts = [example[\"prompt\"] for example in batch]\n",
    "\t\tencoding = self.tokenizer(\n",
    "\t\t\tprompts,\n",
    "\t\t\treturn_tensors=\"pt\",\n",
    "\t\t\tpadding=True,\n",
    "\t\t\tpadding_side=\"left\",\n",
    "\t\t\ttruncation=True,\n",
    "\t\t\tmax_length=self.max_prompt_length,\n",
    "\t\t)\n",
    "\t\treturn {\n",
    "\t\t\t\"input_ids\": encoding[\"input_ids\"],\n",
    "\t\t\t\"attention_mask\": encoding[\"attention_mask\"],\n",
    "\t\t}\n",
    "\n",
    "\tdef train_dataloader(self):\n",
    "\t\treturn DataLoader(\n",
    "\t\t\tself.train_dataset,\n",
    "\t\t\tbatch_size=self.batch_size,\n",
    "\t\t\tshuffle=True,\n",
    "\t\t\tnum_workers=self.num_workers,\n",
    "\t\t\tpersistent_workers=True,\n",
    "\t\t\tcollate_fn=self._collate_fn,\n",
    "\t\t)\n",
    "\n",
    "\tdef val_dataloader(self):\n",
    "\t\treturn DataLoader(\n",
    "\t\t\tself.val_dataset,\n",
    "\t\t\tbatch_size=self.batch_size,\n",
    "\t\t\tshuffle=False,\n",
    "\t\t\tnum_workers=self.num_workers,\n",
    "\t\t\tpersistent_workers=True,\n",
    "\t\t\tcollate_fn=self._collate_fn,\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7887ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceModel(LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself, policy_model, reward_model, learning_rate, max_new_tokens, alpha, top_k, top_p, eos_token_id\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters(ignore=['policy_model', 'reward_model'])\n",
    "\n",
    "\t\tself.policy_model = policy_model\n",
    "\t\tself.reward_model = reward_model\n",
    "\t\tself.reward_model.eval()\n",
    "\n",
    "\t\t# Инициализация бейзлайна в виде скользящего среднего\n",
    "\t\tself.register_buffer(\"moving_avg_reward\", torch.tensor(0.0))\n",
    "\n",
    "\tdef configure_optimizers(self):\n",
    "\t\toptimizer = AdamW(self.policy_model.parameters(), lr=self.hparams.learning_rate)\n",
    "\t\treturn optimizer\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef generate(self, input_ids, attention_mask=None):\n",
    "\t\t# генерация ответов policy модели\n",
    "\t\treturn self.policy_model.generate(\n",
    "\t\t\tinput_ids=input_ids,\n",
    "\t\t\tattention_mask=attention_mask,\n",
    "\t\t\tmax_new_tokens=self.hparams.max_new_tokens,\n",
    "\t\t\tdo_sample=True,\n",
    "\t\t\ttop_k=self.hparams.top_k,\n",
    "\t\t\ttop_p=self.hparams.top_p,\n",
    "\t\t\tpad_token_id=self.hparams.eos_token_id,\n",
    "\t\t)\n",
    "\n",
    "\tdef compute_reward(self, input_ids, attention_mask=None):\n",
    "\t\toutputs = self.reward_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\t\t# возьмем за награду вероятность положительного класса\n",
    "\t\trewards = torch.sigmoid(outputs.logits)\n",
    "\t\treturn rewards\n",
    "\n",
    "\tdef training_step(self, batch, batch_idx):\n",
    "\t\tinput_ids = batch[\"input_ids\"]\n",
    "\t\tattention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "\t\tfull_input_ids = self.generate(input_ids, attention_mask)  # prompt + response\n",
    "\t\tresponses = full_input_ids[:, input_ids.shape[-1]:]  # response only\n",
    "\t\tresponse_attention_mask = torch.where(\n",
    "\t\t\tresponses == self.hparams.eos_token_id,\n",
    "\t\t\ttorch.tensor(1), torch.tensor(0)\n",
    "\t\t)\n",
    "\t\tfull_attention_mask = torch.cat([attention_mask, response_attention_mask], dim=1)\n",
    "\n",
    "\t\t# считаем награды\n",
    "\t\trewards = self.compute_reward(full_input_ids, full_attention_mask).detach()\n",
    "\t\tself.moving_avg_reward = self.hparams.alpha * self.moving_avg_reward + (1 - self.hparams.alpha) * rewards.mean()\n",
    "\n",
    "\t\tadvantages = rewards - self.moving_avg_reward\n",
    "\t\t\n",
    "\t\t# Получаем логиты (задача классификатора) от Policy Model\n",
    "\t\tlogits = self.policy_model(full_input_ids, attention_mask=full_attention_mask).logits\n",
    "\t\tresponse_logits = logits[:, -self.hparams.max_new_tokens-1:-1, :]\n",
    "\t\tlog_probs = response_logits.log_softmax(dim=-1)\n",
    "\t\ttarget_ids = responses.clone()\n",
    "\t\tselected_log_probs = torch.gather(log_probs, 2, target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "\t\tloss = - (selected_log_probs * advantages).mean()\n",
    "\n",
    "\t\tself.log_dict({\n",
    "            \"train_loss\": loss,\n",
    "            \"avg_reward\": rewards.mean(),\n",
    "            \"moving_avg\": self.moving_avg_reward,\n",
    "        })\n",
    "\n",
    "\t\treturn loss\n",
    "\t\n",
    "\tdef validation_step(self, batch, batch_idx):\n",
    "\t\tpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d951ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "    )\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"reward_models/SmolLM2_135_classifier/checkpoint-673\"\n",
    "\t)\n",
    "\n",
    "dm = ReinforceDataModule(\n",
    "    dataset=ds,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=12,\n",
    "    max_prompt_length=192,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "reinforce_model = ReinforceModel(\n",
    "\tpolicy_model = policy_model,\n",
    "\treward_model = reward_model,\n",
    "\tlearning_rate=5e-5,\n",
    "\tmax_new_tokens=512,\n",
    "\talpha=0.99,\n",
    "\ttop_k=30,\n",
    "\ttop_p=0.95,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "logger = WandbLogger(\n",
    "    name=\"smollm2-reinforce\",\n",
    "    save_dir=\"reinforce_models/\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=3,\n",
    "    log_every_n_steps=10,\n",
    "    precision=\"16-mixed\",\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0ca3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m671342ihxrxmx\u001b[0m (\u001b[33m671342ihxrxmx-iu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>reinforce_models/wandb/run-20250704_024848-vfozif6a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/671342ihxrxmx-iu/lightning_logs/runs/vfozif6a' target=\"_blank\">smollm2-reinforce</a></strong> to <a href='https://wandb.ai/671342ihxrxmx-iu/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/671342ihxrxmx-iu/lightning_logs' target=\"_blank\">https://wandb.ai/671342ihxrxmx-iu/lightning_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/671342ihxrxmx-iu/lightning_logs/runs/vfozif6a' target=\"_blank\">https://wandb.ai/671342ihxrxmx-iu/lightning_logs/runs/vfozif6a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type                           | Params | Mode\n",
      "-----------------------------------------------------------------------\n",
      "0 | policy_model | LlamaForCausalLM               | 134 M  | eval\n",
      "1 | reward_model | LlamaForSequenceClassification | 134 M  | eval\n",
      "-----------------------------------------------------------------------\n",
      "269 M     Trainable params\n",
      "0         Non-trainable params\n",
      "269 M     Total params\n",
      "1,076.122 Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "794       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e7a0f8dd4e4e28ad3e0eef29fe0607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68db14154da74d6387a435dfcdf7447c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(reinforce_model, datamodule=dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
